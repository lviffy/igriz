# ============================================
# AI Provider Configuration
# ============================================
# Choose your AI provider: 'openrouter' or 'lmstudio'
# Default: openrouter
AI_PROVIDER=openrouter

# ============================================
# OpenRouter Configuration (Cloud-based LLMs)
# ============================================
# Get your API key from: https://openrouter.ai/keys
# Popular models: google/gemini-2.5-flash, deepseek/deepseek-chat, meta-llama/llama-3.1-70b-instruct
OPEN_ROUTER_API_KEY=your-openrouter-api-key-here

# ============================================
# LM Studio Configuration (Local LLMs)
# ============================================
# To use LM Studio:
# 1. Download and install LM Studio from: https://lmstudio.ai/
# 2. Load a model in LM Studio
# 3. Start the local server (usually runs on http://localhost:1234)
# 4. Set AI_PROVIDER=lmstudio
# 5. Optionally customize the base URL below if using a different port

# LM Studio server URL (default: ws://localhost:1234)
# Note: Must use WebSocket protocol (ws:// or wss://)
# LMSTUDIO_BASE_URL=ws://localhost:1234

# LM Studio model identifier (get this from your loaded model in LM Studio)
# Example: uigen-t1-7b, llama-3.2-3b-instruct, etc.
# LMSTUDIO_MODEL_ID=uigen-t1-7b

